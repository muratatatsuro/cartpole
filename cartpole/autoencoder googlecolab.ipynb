{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "#auto encoder\n",
    "\"\"\"layer,loss,train,evaluate,main\n",
    "   inferenceの部分がencoder,decoderに変換される\"\"\"\n",
    "import numpy as np\n",
    "import time,argparse\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "#正規化\n",
    "x_train=x_train.reshape(-1,784).astype('float32')/255\n",
    "x_test=x_test.reshape(-1,784).astype('float32')/255\n",
    "#onehot\n",
    "y_train=keras.utils.to_categorical(y_train,10)\n",
    "y_test=keras.utils.to_categorical(y_test,10)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape[0])\n",
    "\n",
    "#parameters\n",
    "n_encoder_hidden_1=1000\n",
    "n_encoder_hidden_2=500\n",
    "n_encoder_hidden_3=250\n",
    "n_decoder_hidden_1=250\n",
    "n_decoder_hidden_2=500\n",
    "n_decoder_hidden_3=1000\n",
    "\n",
    "training_epochs=10\n",
    "batch_size=64\n",
    "display_step=1#コンソールに表示させる回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time,argparse\n",
    "#parameters\n",
    "n_encoder_hidden_1=1000\n",
    "n_encoder_hidden_2=500\n",
    "n_encoder_hidden_3=250\n",
    "n_decoder_hidden_1=250\n",
    "n_decoder_hidden_2=500\n",
    "n_decoder_hidden_3=1000\n",
    "\n",
    "training_epochs=10\n",
    "batch_size=64\n",
    "display_step=1#コンソールに表示させる回数\n",
    "def layer(input,weight_shape,bias_shape,phase_train):\n",
    "    weight_init=tf.random_normal_initialzier(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    bias_init=tf.constant_initialzier(value=0)\n",
    "    \n",
    "    W=tf.get_variable('W',weight_shape,initializer=weight_init)\n",
    "    b=tf.get_variable('b',bias_shape,initialzier=bias_init)\n",
    "    \n",
    "    #総入力\n",
    "    logits=tf.matmul(input,W)+b\n",
    "    return tf.nn.sigmoid(layer_norm_batch(logits,weight_shape[1],phase_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#バッチ正規化\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "def layer_batch_norm(x,n_out,phase_train):\n",
    "    beta_init=tf.constant_initializer(value=0.0,dtype=tf.float32)\n",
    "    gamma_init=tf.constant_initializer(value=1.0,dtype=tf.float32)\n",
    "    \n",
    "    beta=tf.get_variable('beta',[n_out],initializer=beta_init)\n",
    "    gamma=tf.get_variable('gamma',[n_out],initializer=gamma_init)\n",
    "    \n",
    "    batch_mean,batch_var=tf.nn.moments(x,[0],name='moments')\n",
    "    \n",
    "    ema=tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op=ema.apply([batch_mean,batch_var])\n",
    "    ema_mean=ema.average(batch_mean)\n",
    "    ema_var=ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        return tf.control_dependencies(tf.identity(batch_mean),tf.identity(batch_var))\n",
    "    \n",
    "    mean,var=control_flow_ops.cond(phase_train,\n",
    "                                  mean_var_with_update,\n",
    "                                  lambda:(ema_mean,ema_var))\n",
    "    x_r=tf.reshape(x,[-1,1,1,n_out])\n",
    "    normed=tf.nn.batch_norm_with_global_normalization(x_r,mean,var,beta,gamma,1e-3,True)\n",
    "    \n",
    "    return tr.reshape(normed,[-1,n_out])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x,n_code,phase_train):\n",
    "    with tf.variable_scope(\"encode\"):\n",
    "        with tf.variable_scope(\"hidden1\"):\n",
    "            hidden_1=layer(x,\n",
    "                           [784,n_encoder_hidden_1],\n",
    "                           [n_encoder_hidden_1],\n",
    "                           phase_train)\n",
    "            \n",
    "        with tf.variable_scope(\"hidden2\"):\n",
    "            hidden_2=layer(hidden_1,\n",
    "                          [n_encoder_hidden_1,n_encoder_hidden_2],\n",
    "                          [n_encoder_hidden_2],\n",
    "                          phase_train)\n",
    "        with tf.variable_scope(\"hidden3\"):\n",
    "            hidden_3=layer(hidden_2,\n",
    "                          [n_encoder_hidden2,n_encoder_hidden_3],\n",
    "                          [n_encoder_hidden_3],\n",
    "                          phase_train)\n",
    "            \n",
    "        with tf.variable_scope(\"code\"):\n",
    "            code=layer(hidden_3,\n",
    "                       [n_encoder_hidden_3,n_code],\n",
    "                       [n_code],\n",
    "                      phase_train)\n",
    "        return code\n",
    "    \n",
    "def decoder(code,n_code,phase_train):\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        with tf.variable_scope(\"hidden1\"):\n",
    "            hidden_1=layer(code,[n_code,n_decoder_hidden_1],[n_decoder_hidden_1],phase_train)\n",
    "        with tf.variable_scope(\"hidden2\"):\n",
    "            hidden_2=layer(hidden_1,[n_decoder_hidden_1,n_decoder_hidden_2],[n_decoder_hidden_2],phase_train)\n",
    "        with tf.variable_scope(\"hidden3\"):\n",
    "            hidden_3=layer(hidden_2,[n_decoder_hidden_2,n_decoder_hidden_3],[n_decoder_hidden_3],phase_train)\n",
    "            \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output=layer(hidden_3,[n_decoder_hidden_3,784],[784],phase_train)\n",
    "            \n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss,train,evaluate\n",
    "def loss(output,x):\n",
    "    #l2norm\n",
    "    l2=tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(output,x)),1))\n",
    "    train_loss=tf.reduce_mean(l2)\n",
    "    train_summary_op=tf.summary.scalar(\"train_cost\",train_loss)\n",
    "    return train_loss,train_summary_op\n",
    "\n",
    "def train(cost,global_step):\n",
    "    optimizer=tf.train.AdamOptimizer(lr=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,\n",
    "                                    use_locking=False,name='Adam')\n",
    "    train_op=optimizer.minimize(cost,global_step=global_step)\n",
    "    return train_op\n",
    "#?\n",
    "def image_summary(summary_label,tensor):\n",
    "    tensor_reshaped=tf.reshape(tensor,[-1,28,28,1])\n",
    "    return tf.summary.image(summary_label,tensor_reshaped)\n",
    "\n",
    "def evaluate(output,x):\n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        in_im_op=image_summary(\"input_image\",x)\n",
    "        out_im_op=image_summary(\"output_image\",output)\n",
    "        l2=tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(output,x,name=\"val_diff\")),1))\n",
    "        val_loss=tf.reduce_mean(l2)\n",
    "        val_summary_op=tf.summary.scalar(\"val_cost\",val_loss)\n",
    "        \n",
    "        return val_loss,in_im_op,out_im_op,val_summary_op\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(x,y,batch_size):\n",
    "    idx=np.arange(y_train.shape[0])\n",
    "    \n",
    "    np.random.shuffle(idx)\n",
    "    x=x[idx]\n",
    "    y=y[idx]\n",
    "    \n",
    "    for i in range(0,x.shape[0],batch_size):\n",
    "        yield(x[i:i+batch_size,:],y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] n_code\n",
      "ipykernel_launcher.py: error: argument n_code: invalid int value: 'C:\\\\Users\\\\murata\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-2307f4ae-9546-4133-bccf-1b0b90eb82d3.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser(description=\"Test various optimization strategies\")\n",
    "    parser.add_argument(\"n_code\",type=int)\n",
    "    args=parser.parse_args()\n",
    "    n_code=args.n_code\n",
    "    log_dir=\"mnist_auto_encoder_hidden={}_logs/\".format(n_code)\n",
    "    #x_train,x_testを書く\n",
    "    (x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "    x_train=x_train.reshape(-1,784).astype('float32')/255\n",
    "    x_test=x_test.reshape(-1,784).astype('float32')/255\n",
    "    \n",
    "    y_train=keras.utils.to_categorical(y_train,10)\n",
    "    y_test=keras.utils.to_categorical(y_test,10)\n",
    "    \n",
    "    g=tf.Graph()\n",
    "    \n",
    "    with g.as_default():\n",
    "        with tf.variable_scope(\"autoencoder_model\"):\n",
    "            #mnist data shape 28*28=784\n",
    "            x=tf.placeholder('float',[None,784])\n",
    "            phase_train=tf.placeholder(tf.bool)\n",
    "            \n",
    "            code=encoder(x,n_code,phase_train)\n",
    "            output=deocoder(code,n_code,phase_train)\n",
    "            \n",
    "            cost,train_summary_op=loss(output,x)\n",
    "            global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "            train_op=train(cost,global_step)\n",
    "            eval_op,in_im_op,out_im_op,val_summary_op=evaluate(output,x)\n",
    "            \n",
    "            summary_op=tf.summary.merge_all()#?\n",
    "            \n",
    "            saver=tf.train.Saver()\n",
    "            sess=tf.Session()\n",
    "            summary_writer=tf.summary.FileWriter(log_dir,graph=sess.graph)\n",
    "            \n",
    "            #初期化\n",
    "            init_op=tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            \n",
    "            #training\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_cost=0.0\n",
    "                batch_gen=create_batch(x_train,y_train,batch_size)\n",
    "                total_batch=int(x_train.shape[0]/batch_size)\n",
    "                for idx,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "                    #fitting\n",
    "                    _,new_cost,train_summary=sess.run([train_op,cost,train_summary_op],\n",
    "                                                     feed_dict={x:batch_x,phase_train:True})\n",
    "                    summary_writer.add_summary(train_summary,sess.run(global_step))\n",
    "                    #costを計算\n",
    "                    avg_cost+=new_cost/total_batch\n",
    "                    \n",
    "                if epoch%display_step==0:\n",
    "                    print(\"epoch:%04d   cost:%.4f\"%(epoch+1,avg_cost))\n",
    "                    \n",
    "                    summary_writer.add_summary(train_summary,sess.run(global_step))\n",
    "                    \n",
    "                    validation_loss,in_im,out_im,val_summary=sess.run([eval_op,in_im_op,out_im_op,val_summary_op],\n",
    "                                                                     feed_dict={x:x_test,phase_train:False})\n",
    "                    summary_writer.add_summary(in_im,sess.run(global_step))\n",
    "                    summary_writer.add_summary(out_im,sess.run(global_step))\n",
    "                    summary_writer.add_summary(val_summary,sess.run(global_step))\n",
    "                    \n",
    "                    print('validation acc:%.3f'%(validation_loss))\n",
    "                    \n",
    "                    saver.save(sess,\"{}/model-checkpoints-{:04d}\".format(log_dir,epoch+1),global_step=global_step)\n",
    "                    \n",
    "                print('==optimization finished==')\n",
    "                \n",
    "                test_loss=sess.run([eval_op],feed_dict={x:x_test,phase_train:False})\n",
    "                \n",
    "                print('test loss:',test_loss)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
